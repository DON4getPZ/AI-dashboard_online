"""
ë©”ëª¨ë¦¬ ìµœì í™” ë§ˆì¼€íŒ… ë°ì´í„° ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸

ë©”ëª¨ë¦¬ ìµœì í™” ê¸°ë²•:
1. ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ì½ê¸°/ì²˜ë¦¬
2. ë°ì´í„° íƒ€ì… ìµœì í™” (float64 â†’ float32, object â†’ category)
3. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
4. Prophet ëª¨ë¸ ìˆœì°¨ í•™ìŠµ í›„ ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
5. ì¤‘ê°„ ë°ì´í„°í”„ë ˆì„ ì¦‰ì‹œ ì‚­ì œ
6. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

í™˜ê²½ë³€ìˆ˜:
- INPUT_CSV_PATH: ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: raw_data.csv)
"""

import os
import sys
import json
import gc
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any
import warnings

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import pandas as pd
import numpy as np
from scipy import stats
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns

# Prophet
try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
except ImportError:
    PROPHET_AVAILABLE = False
    print("âš ï¸ Prophetì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. ë‹¨ìˆœ ì˜ˆì¸¡ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

warnings.filterwarnings('ignore')

# ë””ë ‰í† ë¦¬ ì„¤ì •
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / 'data'
RAW_DIR = DATA_DIR / 'raw'
META_DIR = DATA_DIR / 'meta'
FORECAST_DIR = DATA_DIR / 'forecast'
STATS_DIR = DATA_DIR / 'statistics'
VISUAL_DIR = DATA_DIR / 'visualizations'

for dir_path in [RAW_DIR, META_DIR, FORECAST_DIR, STATS_DIR, VISUAL_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False


def get_memory_usage():
    """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ (MB)"""
    try:
        import psutil
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024  # MB
    except ImportError:
        return -1


def optimize_dtypes(df: pd.DataFrame) -> pd.DataFrame:
    """ë°ì´í„° íƒ€ì… ìµœì í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½"""
    print("ğŸ”§ ë°ì´í„° íƒ€ì… ìµœì í™” ì¤‘...")

    original_mem = df.memory_usage(deep=True).sum() / 1024 / 1024

    # ìˆ«ìí˜• ì»¬ëŸ¼ ìµœì í™”
    for col in df.select_dtypes(include=['float64']).columns:
        df[col] = df[col].astype('float32')

    for col in df.select_dtypes(include=['int64']).columns:
        df[col] = df[col].astype('int32')

    # ë¬¸ìì—´ ì»¬ëŸ¼ì„ categoryë¡œ ë³€í™˜ (ë°˜ë³µë˜ëŠ” ê°’ì´ ë§ì€ ê²½ìš°)
    for col in df.select_dtypes(include=['object']).columns:
        num_unique_values = len(df[col].unique())
        num_total_values = len(df[col])
        if num_unique_values / num_total_values < 0.5:  # 50% ì´í•˜ë©´ categoryë¡œ
            df[col] = df[col].astype('category')

    optimized_mem = df.memory_usage(deep=True).sum() / 1024 / 1024
    saved = original_mem - optimized_mem

    print(f"   â”œ ìµœì í™” ì „: {original_mem:.2f} MB")
    print(f"   â”œ ìµœì í™” í›„: {optimized_mem:.2f} MB")
    print(f"   â”” ì ˆì•½: {saved:.2f} MB ({saved/original_mem*100:.1f}%)")

    return df


def load_and_clean_data_chunked(file_path: str) -> pd.DataFrame:
    """ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
    print("ğŸ“¥ ë°ì´í„° ë¡œë”© ì¤‘ (ì²­í¬ ëª¨ë“œ)...")

    mem_before = get_memory_usage()
    if mem_before > 0:
        print(f"   â”œ ì‹œì‘ ë©”ëª¨ë¦¬: {mem_before:.1f} MB")

    # ì²« ë²ˆì§¸ ì²­í¬ë¡œ ì»¬ëŸ¼ í™•ì¸
    sample = pd.read_csv(file_path, encoding='utf-8', nrows=100)
    print(f"   â”œ ì»¬ëŸ¼ ìˆ˜: {len(sample.columns)}")
    print(f"   â”” ì»¬ëŸ¼ëª…: {list(sample.columns[:5])}")

    # ì²­í¬ í¬ê¸° ê²°ì • (ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
    chunk_size = 5000
    chunks = []

    try:
        for i, chunk in enumerate(pd.read_csv(file_path, encoding='utf-8', chunksize=chunk_size)):
            # ì²­í¬ë³„ ë©”ëª¨ë¦¬ ìµœì í™”
            chunk = optimize_dtypes(chunk)
            chunks.append(chunk)

            if (i + 1) % 10 == 0:
                print(f"   â”œ ì²˜ë¦¬ëœ ì²­í¬: {i+1} ({len(chunk) * (i+1):,} rows)")
                gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    except UnicodeDecodeError:
        print("   â”œ UTF-8 ì‹¤íŒ¨, CP949 ì‹œë„...")
        for chunk in pd.read_csv(file_path, encoding='cp949', chunksize=chunk_size):
            chunk = optimize_dtypes(chunk)
            chunks.append(chunk)
            gc.collect()

    # ì²­í¬ ë³‘í•©
    print("   â”œ ì²­í¬ ë³‘í•© ì¤‘...")
    df = pd.concat(chunks, ignore_index=True)
    del chunks  # ë©”ëª¨ë¦¬ í•´ì œ
    gc.collect()

    print(f"   â”œ ë¡œë“œëœ í–‰ ìˆ˜: {len(df):,}")

    mem_after = get_memory_usage()
    if mem_after > 0:
        print(f"   â”” í˜„ì¬ ë©”ëª¨ë¦¬: {mem_after:.1f} MB (ì¦ê°€: +{mem_after - mem_before:.1f} MB)")

    return df


def forecast_with_memory_control(daily_df: pd.DataFrame, days: int = 30) -> pd.DataFrame:
    """ë©”ëª¨ë¦¬ ì œì–´ê°€ ê°€ëŠ¥í•œ ì˜ˆì¸¡ í•¨ìˆ˜"""
    print("ğŸ”® ì˜ˆì¸¡ ìƒì„± ì¤‘ (ë©”ëª¨ë¦¬ ìµœì í™”)...")

    if not PROPHET_AVAILABLE:
        return simple_forecast(daily_df, days)

    metrics = ['ë¹„ìš©', 'ë…¸ì¶œ', 'í´ë¦­', 'ì „í™˜ìˆ˜', 'ì „í™˜ê°’']
    all_predictions = []

    mem_start = get_memory_usage()

    for idx, metric in enumerate(metrics):
        print(f"   â”œ [{idx+1}/{len(metrics)}] {metric} ì˜ˆì¸¡ ì¤‘...")

        try:
            # Prophet ë°ì´í„° ì¤€ë¹„ (float32 ì‚¬ìš©)
            prophet_df = daily_df[['ì¼ êµ¬ë¶„', metric]].copy()
            prophet_df.columns = ['ds', 'y']
            prophet_df['y'] = prophet_df['y'].astype('float32')

            # Prophet ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
            model = Prophet(
                yearly_seasonality=False,
                weekly_seasonality=True,
                daily_seasonality=False,
                seasonality_mode='additive',
                changepoint_prior_scale=0.05
            )
            model.fit(prophet_df)

            # ì˜ˆì¸¡
            future = model.make_future_dataframe(periods=days)
            forecast = model.predict(future)

            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ (ë©”ëª¨ë¦¬ ì ˆì•½)
            forecast = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(days).copy()
            forecast.columns = ['ì¼ êµ¬ë¶„', f'{metric}_ì˜ˆì¸¡', f'{metric}_í•˜í•œ', f'{metric}_ìƒí•œ']
            forecast[f'{metric}_ì˜ˆì¸¡'] = forecast[f'{metric}_ì˜ˆì¸¡'].astype('float32')
            forecast[f'{metric}_í•˜í•œ'] = forecast[f'{metric}_í•˜í•œ'].astype('float32')
            forecast[f'{metric}_ìƒí•œ'] = forecast[f'{metric}_ìƒí•œ'].astype('float32')

            all_predictions.append(forecast)

            # ëª¨ë¸ ì¦‰ì‹œ ì‚­ì œ ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            del model, prophet_df, future
            gc.collect()

            mem_current = get_memory_usage()
            if mem_current > 0:
                print(f"      ë©”ëª¨ë¦¬: {mem_current:.1f} MB")

        except Exception as e:
            print(f"   â”œ âš ï¸ {metric} ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ ì˜ˆì¸¡ìœ¼ë¡œ ëŒ€ì²´
            simple_pred = simple_forecast_single(daily_df, metric, days)
            all_predictions.append(simple_pred)

    # ëª¨ë“  ì˜ˆì¸¡ ë³‘í•©
    result = all_predictions[0]
    for pred in all_predictions[1:]:
        result = result.merge(pred, on='ì¼ êµ¬ë¶„', how='outer')

    del all_predictions
    gc.collect()

    mem_end = get_memory_usage()
    if mem_end > 0 and mem_start > 0:
        print(f"   â”” ë©”ëª¨ë¦¬ ë³€í™”: {mem_start:.1f} MB â†’ {mem_end:.1f} MB")

    return result


def simple_forecast_single(df: pd.DataFrame, metric: str, days: int = 30) -> pd.DataFrame:
    """ë‹¨ì¼ ë©”íŠ¸ë¦­ì— ëŒ€í•œ ê°„ë‹¨í•œ ì˜ˆì¸¡"""
    daily = df.groupby('ì¼ êµ¬ë¶„')[metric].sum().reset_index()
    daily['ì¼ êµ¬ë¶„'] = pd.to_datetime(daily['ì¼ êµ¬ë¶„'])
    daily = daily.sort_values('ì¼ êµ¬ë¶„')

    # ìµœê·¼ ë°ì´í„° ê¸°ë°˜ ì˜ˆì¸¡
    recent_avg = daily[metric].tail(30).mean()

    last_date = daily['ì¼ êµ¬ë¶„'].max()
    future_dates = pd.date_range(start=last_date + timedelta(days=1), periods=days, freq='D')

    predictions = pd.DataFrame({
        'ì¼ êµ¬ë¶„': future_dates,
        f'{metric}_ì˜ˆì¸¡': recent_avg,
        f'{metric}_í•˜í•œ': recent_avg * 0.8,
        f'{metric}_ìƒí•œ': recent_avg * 1.2
    })

    for col in [f'{metric}_ì˜ˆì¸¡', f'{metric}_í•˜í•œ', f'{metric}_ìƒí•œ']:
        predictions[col] = predictions[col].astype('float32')

    return predictions


def simple_forecast(df: pd.DataFrame, days: int = 30) -> pd.DataFrame:
    """ê°„ë‹¨í•œ ì˜ˆì¸¡ (Prophet ì—†ì´)"""
    print("   â”œ ê°„ë‹¨í•œ í†µê³„ ê¸°ë°˜ ì˜ˆì¸¡ ì‚¬ìš©")

    metrics = ['ë¹„ìš©', 'ë…¸ì¶œ', 'í´ë¦­', 'ì „í™˜ìˆ˜', 'ì „í™˜ê°’']
    all_predictions = []

    for metric in metrics:
        pred = simple_forecast_single(df, metric, days)
        all_predictions.append(pred)

    result = all_predictions[0]
    for pred in all_predictions[1:]:
        result = result.merge(pred, on='ì¼ êµ¬ë¶„', how='outer')

    return result


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸš€ ë§ˆì¼€íŒ… ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ (ë©”ëª¨ë¦¬ ìµœì í™” ë²„ì „)")
    print("=" * 80)
    print()

    # ì‹œì‘ ì‹œ ë©”ëª¨ë¦¬ ìƒíƒœ
    mem_initial = get_memory_usage()
    if mem_initial > 0:
        print(f"ğŸ’¾ ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {mem_initial:.1f} MB")
        print()

    # ì…ë ¥ íŒŒì¼ ê²½ë¡œ
    input_file = os.environ.get('INPUT_CSV_PATH', 'raw_data.csv')

    if not os.path.exists(input_file):
        print(f"âŒ ì˜¤ë¥˜: {input_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # 1. ë°ì´í„° ë¡œë“œ (ì²­í¬ ëª¨ë“œ)
    df = load_and_clean_data_chunked(input_file)

    # ì»¬ëŸ¼ëª… ì •ë¦¬
    df.columns = df.columns.str.strip()

    # 2. ë‚ ì§œ ì²˜ë¦¬
    print("\nğŸ“… ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬ ì¤‘...")
    df['ì¼ êµ¬ë¶„'] = pd.to_datetime(df['ì¼ êµ¬ë¶„'], errors='coerce')
    df = df.dropna(subset=['ì¼ êµ¬ë¶„'])

    # 3. ì›”ë³„ ë°ì´í„° ì €ì¥
    print("\nğŸ’¾ ì›”ë³„ ë°ì´í„° ì €ì¥ ì¤‘...")
    for year_month, group in df.groupby(df['ì¼ êµ¬ë¶„'].dt.to_period('M')):
        output_file = RAW_DIR / f"{year_month}.csv"
        # float32ë¡œ ì €ì¥í•˜ì—¬ íŒŒì¼ í¬ê¸°ë„ ì ˆì•½
        group.to_csv(output_file, index=False, encoding='utf-8')
        print(f"   â”œ {output_file.name}: {len(group):,} rows")

    gc.collect()

    # 4. í†µê³„ ìƒì„±
    print("\nğŸ“Š í†µê³„ ìƒì„± ì¤‘...")
    daily_stats = df.groupby('ì¼ êµ¬ë¶„').agg({
        'ë¹„ìš©': 'sum',
        'ë…¸ì¶œ': 'sum',
        'í´ë¦­': 'sum',
        'ì „í™˜ìˆ˜': 'sum',
        'ì „í™˜ê°’': 'sum'
    }).reset_index()

    # float32 ë³€í™˜
    for col in ['ë¹„ìš©', 'ë…¸ì¶œ', 'í´ë¦­', 'ì „í™˜ìˆ˜', 'ì „í™˜ê°’']:
        daily_stats[col] = daily_stats[col].astype('float32')

    stats_summary = {
        'total_days': len(daily_stats),
        'date_range': {
            'start': daily_stats['ì¼ êµ¬ë¶„'].min().strftime('%Y-%m-%d'),
            'end': daily_stats['ì¼ êµ¬ë¶„'].max().strftime('%Y-%m-%d')
        },
        'totals': {
            'ë¹„ìš©': float(daily_stats['ë¹„ìš©'].sum()),
            'ë…¸ì¶œ': int(daily_stats['ë…¸ì¶œ'].sum()),
            'í´ë¦­': int(daily_stats['í´ë¦­'].sum()),
            'ì „í™˜ìˆ˜': int(daily_stats['ì „í™˜ìˆ˜'].sum()),
            'ì „í™˜ê°’': float(daily_stats['ì „í™˜ê°’'].sum())
        },
        'averages': {
            'ë¹„ìš©': float(daily_stats['ë¹„ìš©'].mean()),
            'ë…¸ì¶œ': float(daily_stats['ë…¸ì¶œ'].mean()),
            'í´ë¦­': float(daily_stats['í´ë¦­'].mean()),
            'ì „í™˜ìˆ˜': float(daily_stats['ì „í™˜ìˆ˜'].mean()),
            'ì „í™˜ê°’': float(daily_stats['ì „í™˜ê°’'].mean())
        }
    }

    with open(STATS_DIR / 'statistics.json', 'w', encoding='utf-8') as f:
        json.dump(stats_summary, f, ensure_ascii=False, indent=2)

    print("   â”” í†µê³„ ì €ì¥ ì™„ë£Œ")

    # 5. ì˜ˆì¸¡ ìƒì„±
    print("\nğŸ”® ì˜ˆì¸¡ ìƒì„± ì¤‘...")
    predictions = forecast_with_memory_control(df, days=30)

    # ì˜ˆì¸¡ ì €ì¥
    predictions.to_csv(FORECAST_DIR / 'predictions_daily.csv', index=False, encoding='utf-8')
    print("   â”” ì˜ˆì¸¡ ì €ì¥ ì™„ë£Œ")

    # 6. ë©”íƒ€ë°ì´í„°
    metadata = {
        'last_updated': datetime.now().isoformat(),
        'data_source': input_file,
        'total_records': len(df),
        'date_range': stats_summary['date_range'],
        'optimization': 'memory_optimized',
        'prophet_used': PROPHET_AVAILABLE
    }

    with open(META_DIR / 'latest.json', 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
    del df, daily_stats, predictions
    gc.collect()

    mem_final = get_memory_usage()

    print("\n" + "=" * 80)
    print("âœ… ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 80)
    if mem_initial > 0 and mem_final > 0:
        print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {mem_initial:.1f} MB â†’ {mem_final:.1f} MB")
        print(f"   í”¼í¬ ë©”ëª¨ë¦¬ ì¦ê°€: +{mem_final - mem_initial:.1f} MB")
    print()


if __name__ == '__main__':
    try:
        main()
    except MemoryError:
        print("\n" + "=" * 80)
        print("âŒ ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜!")
        print("=" * 80)
        print("\ní•´ê²° ë°©ë²•:")
        print("1. 64-bit Python ì‚¬ìš© (ì¶”ì²œ)")
        print("2. process_lite.bat ì‹¤í–‰ (ë” ê°€ë²¼ìš´ ë²„ì „)")
        print("3. ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ í›„ ì¬ì‹œë„")
        print("4. ë°ì´í„°ë¥¼ ì‘ì€ ê¸°ê°„ìœ¼ë¡œ ë¶„í• ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
